---
title: "Prediction Model"
author: "Phong, Marvin, Isabel"
date: "2025-03-08"
output: html_document
---W
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```

# Objectives

Machine learning techniques will be applied to forecast support levels <https://github.com/phongtdng/Coursework---Text-Mining>based on observed trends.

Ensure the modelâ€™s accuracy and reliability, rigorous calibration and validation will be necessary.

# Library

```{r}
library(tidyverse)
library(ggplot2)
library(caret)
library(ROSE)
library(corrplot)
library(glmnet)
library(lme4)
```

# Data

```{r}
# data_original <- haven::read_dta("../data/raw/ZA7575.dta")
# data <- read_rds("../df_rf_enriched_new.rds") %>% 
#   mutate(qa5b_12 = ifelse(is.na(qa5b_12), 12,qa5b_12)) %>% 
#   select(-1)
# 
# data$qc19 <- as.factor(data_original$qc19)
# 
# data <- data %>% 
#   mutate(qc19 = ifelse(qc19 == 3, NA, qc19)) %>% 
#   drop_na(qc19)

data <- read_rds("data_for_ML.rds")
```

Data for training ML is created by imputing questions with "DK" or missing answers. For qa5b specifically, the data is imputed with 12 since respondent's answer as 12 or 13 for qa5a automatically skip question qa5b. Data is then dropped for rows with "DK" answers in qc19.

# Train Model

## Training Data Split

```{r}
set.seed(3147)
trainIndex <- createDataPartition(data$qc19, p=0.7,
                             list = FALSE,
                             times = 1)
train <- data[trainIndex,]
test <- data[-trainIndex,]
```

## Data Imbalance Check

```{r}
train %>% 
  mutate(isocntry = ifelse(isocntry %in% c("DE-W", "DE-E"), "DE", isocntry)) %>% 
  group_by(qc19) %>% 
  summarise(count = n()) %>% 
  mutate(percent = count/sum(count)) %>% 
  arrange(desc(percent)) %>% 
  ungroup() %>% 
  ggplot(aes(x= qc19, y=percent, label=count)) +
  geom_col() +
  geom_text(vjust = -0.3)
```

There is a clear imbalance within the answers to qc19. We, therefore, will balance the data set in order to improve the performance of the prediction model.

## Regularization

### Correlation Check

```{r}
cor <- cor(train %>% select_if(is.numeric))

corrplot(cor, method="color", type="lower", tl.cex = 0.2)
```

### Lasso Dimension Reduction

```{r}
# set.seed(5472) 
# 10-fold cross-validation to select lambda 
# lambdas <- 10^seq(-3,5, length.out=100) 

#Matrix of predictors
X <- train %>% select(-qc19) %>% as.matrix()
y <- train %>% 
  select(qc19) %>% 
  mutate(qc19 = as.numeric(qc19)) %>% 
  scale(center=TRUE, scale=FALSE) %>% 
  as.matrix()

# Lasso with Cross-validation 
# lasso <- cv.glmnet(X,  y,  
#                    alpha = 1, lambda = lambdas, 
#                    standardize = TRUE, nfolds = 10) 
# Plot cv results 
# plot(lasso)`
```

```{r}
# Best cv lambda 
# lambda_cv <- lasso$lambda.min
# Fit final model 
# model_cv <- glmnet(X, y, alpha = 1, lambda = lambda_cv, standardize = TRUE) 
# y_hat_cv <- predict(model_cv, X) # ssr_cv <- t(y-y_hat_cv) %*% (y - y_hat_cv) #  # # Check R-squared # rsq_lasso_cv <- cor(y, y_hat_cv)^2 
# rsq_lasso_cv
```

```{r}
# lasso_coefs <- coef(model_cv)
# 
# select_features_lasso <- rownames(lasso_coefs)[coefs[,1] != 0][-1]
# select_features_lasso
```

### Elastic Net Dimension Reduction

```{r message = FALSE}
set.seed(3244)
# Training control
ctrl <- trainControl(method = "repeatedcv",
                     number=5,
                     repeats = 3,
                     search = "random",
                     verboseIter = TRUE)
# Train model
enet <- train(qc19 ~., data = train, method="glmnet",
              ppreProcess = c("center", "scale"),
              tuneLength = 10, 
              trControl = ctrl)
saveRDS(enet, "regularization/elasticNet.rds")
# Check R-Squared
y_hat_enet <- predict(enet, select(train, -qc19))
rsq_enet <- cor(y, as.numeric(y_hat_enet))^2
rsq_enet
```

```{r}
# Columns with coef = 0
enet_coefs <- coef(enet$finalModel, enet$bestTune$lambda) %>% 
  unlist() 

coef_matrix <- cbind(
  as.matrix(enet_coefs[[1]]),
  as.matrix(enet_coefs[[2]]),
  as.matrix(enet_coefs[[3]])
)
  
select_features_enet <- enet_coefs %>% 
  filter(s1 != 0) %>% 
  row.names() %>% 
  .[-1]

select_features_enet
```

## Feature Selection

```{r}
# train_lasso <- train %>% 
#   select(all_of(c(select_features_lasso, "qc19")))

# Mapping enet
select <- c()
for (i in colnames(train %>% select_if(is.character))) {
  if (sum(grepl("isocntry", select_features_enet)) >0) {
    select <- c(select,i)
  }
}

train_enet <- train %>% 
  select(any_of(c(select_features_enet, select, "qc19"))) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(qc19 = as.factor(qc19))
```

## Model Training

```{r}
ctrl <- trainControl(method = "repeatedcv", number=5,
                     repeats = 5, verboseIter = TRUE)
```

### Random Forest (RF)

```{r}
# enet

## under
ctrl$sampling <- "down"
rf_enet_under <- train(
  qc19 ~ .,
  data = train_enet,
  method = "rf",
  trControl = ctrl
)
saveRDS(rf_enet_under, file = "models/rf_enet_under.rds")

## upper
ctrl$sampling <- "up"
rf_enet_upper <- train(
  qc19 ~ .,
  data = train_enet,
  method = "rf",
  trControl = ctrl
)
saveRDS(rf_enet_upper, file = "models/rf_enet_upper.rds")

## synthetic
ctrl$sampling <- "smote"
rf_enet_smote <- train(
  qc19 ~ .,
  data = train_enet,
  method = "rf",
  trControl = ctrl
)
saveRDS(rf_enet_smote, file = "models/rf_enet_smote.rds")

```

### Gradient Boosting Machines (GBM)

```{r}
# enet

## under
ctrl$sampling <- "down"
gbm_enet_under <- train(
  qc19 ~ .,
  data = train_enet,
  method = "gbm",
  trControl = ctrl
)
saveRDS(gbm_enet_under, file = "models/gbm_enet_under.rds")

## upper
ctrl$sampling <- "up"
gbm_enet_upper <- train(
  qc19 ~ .,
  data = train_enet,
  method = "gbm",
  trControl = ctrl
)
saveRDS(gbm_enet_upper, file = "models/gbm_enet_upper.rds")

## synthetic
ctrl$sampling <- "smote"
gbm_enet_smote <- train(
  qc19 ~ .,
  data = train_enet,
  method = "gbm",
  trControl = ctrl
)
saveRDS(gbm_enet_smote, file = "models/gbm_enet_smote.rds")
```

### Support Vector Machine (SVM)

```{r}
# enet

## under
ctrl$sampling <- "down"
svm_enet_under <- train(
  qc19 ~ .,
  data = train_enet,
  method = "svmRadial",
  trControl = ctrl
)
saveRDS(svm_enet_under, file = "models/svm_enet_under.rds")

## upper
ctrl$sampling <- "up"
svm_enet_upper <- train(
  qc19 ~ .,
  data = train_enet,
  method = "svmRadial",
  trControl = ctrl
)
saveRDS(svm_enet_upper, file = "models/svm_enet_upper.rds")

## synthetic
ctrl$sampling <- "smote"
svm_enet_smote <- train(
  qc19 ~ .,
  data = train_enet,
  method = "svmRadial",
  trControl = ctrl
)
saveRDS(svm_enet_smote, file = "models/svm_enet_smote.rds")

```

# Performance Evaluation

## Performance Evaluation on Training Data

```{r}
# Random Forest
rf_under <- read_rds("models/rf_enet_under.rds")
rf_over <- read_rds("models/rf_enet_upper.rds")
# rf_smote <- read_rds("models/rf_enet_smote.rds")

# Gradient Boosting Machines
gbm_under <- read_rds("models/gbm_enet_under.rds")
gbm_over <- read_rds("models/gbm_enet_upper.rds")
gbm_smote <- read_rds("models/gbm_enet_smote.rds")


# Support Vector Machine
svm_under <- read_rds("models/svm_enet_under.rds")
# svm_over <- read_rds("models/svm_enet_upper.rds")
# svm_smote <- read_rds("models/svm_enet_smote.rds")


results_train <- resamples(list(
  rf_u = rf_under,
  rf_o = rf_over,
  # rf_s =  rf_smote,
  gbm_u = gbm_under,
  gbm_o = gbm_over,
  gbm_s = gbm_smote,
  # svm_o = svm_over,
  # svm_s = svm_smote,
  svm_u = svm_under
  ))  

summary(results_train)
```

```{r}
bwplot(results_train, scales = list(x = list(relation = "free"), y = list(relation = "free")))
```

## Prediction

### Individual Level

```{r}
test <- test %>% 
  mutate(qc19 = as.factor(qc19))

# Random Forest
p_rf_u <- predict(rf_under, test) 
p_rf_o <- predict(rf_over, test)
# p_rf_s <- predict(rf_smote, test)

# Gradient Boosting Machines
p_gbm_u <- predict(gbm_under, test) 
p_gbm_o <- predict(gbm_over, test)
p_gbm_s <- predict(gbm_smote, test)

# Support Vector Machine
p_svm_u <- predict(svm_under, test) 
# p_svm_o <- predict(svm_over, test)
# p_svm_s <- predict(svm_smote, test)

p_list <- list(rf_u = p_rf_u, 
            rf_o = p_rf_o,
            #rf_s = p_rf_s,
            gbm_u = p_gbm_u,
            gbm_o = p_gbm_o,
            gbm_s = p_gbm_s,
            #svm_s = p_svm_s,
            #svm_o = p_svm_o,
            svm_u = p_svm_u)

cMatrix <- as.data.frame(list(
  Model = NA,
  Accuracy = NA,
  Kappy = NA
))

for (i in 1:length(p_list)) {
  model <- names(p_list[i])
  pred <- p_list[i] %>% unlist()
  matrix <- confusionMatrix(pred, test$qc19)$overall[1:2]
  
}
```

### Country Level

```{r}
country_test <- test 
country_test$p_rf_eu <- p_rf_eu
country_test$p_rf_eo <- p_rf_eo

original <- country_test %>% 
  group_by(isocntry, qc19) %>%
  summarise(count = n()) %>% 
  group_by(isocntry) %>% 
  mutate(pcg = count/sum(count))

country_rfeu <- country_test %>% 
  group_by(isocntry, p_rf_eu) %>%
  summarise(count = n()) %>% 
  group_by(isocntry) %>% 
  mutate(pcg = count/sum(count))
country_rfeu

country_rfeo <- country_test %>% 
  group_by(isocntry, p_rf_eo) %>%
  summarise(count = n()) %>% 
  group_by(isocntry) %>% 
  mutate(pcg = count/sum(count))
country_rfeo

comparison <- cbind(select(original, c(isocntry,qc19, pcg)), country_rfeu$pcg, country_rfeo$pcg) %>% 
  rename("response" = 2,
         "original" = 3,
         "rfeu" = 4,
         "rfeo" = 5)
comparison
```
