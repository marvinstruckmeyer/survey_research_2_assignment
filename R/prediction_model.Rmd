---
title: "Prediction Model"
author: "Phong, Marvin, Isabel"
date: "2025-03-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```

# Objectives

Machine learning techniques will be applied to forecast support levels <https://github.com/phongtdng/Coursework---Text-Mining>based on observed trends.

Ensure the modelâ€™s accuracy and reliability, rigorous calibration and validation will be necessary.

# Library

```{r}
library(tidyverse)
library(ggplot2)
library(caret)
library(ROSE)
library(corrplot)
library(glmnet)
```

# Data

```{r}
data <- read_rds("../df_rf_enriched.rds")
```

# Select Modeling Method

Since the data is nested, that is we have responses from individuals from various country, and we aim to analyze the differences across the countries as well as their influence on individuals, we will use a Multilevel Model.

We will also examine other methods such as Random Forest, although the model performance might be compromised as it does not account for the hierarchical structure of the data. We will mitigate this by using a Mixed-Effects Random Forest (MERF), which combines Random Forest and Multilevel Modeling.

# Train Model

## Training Data Split

```{r}
set.seed(3147)
trainIndex <- createDataPartition(data$qc19, p=0.7,
                             list = FALSE,
                             times = 1)
train <- data[trainIndex,]
test <- data[-trainIndex,]
```

## Data Imbalance Check

```{r}
train %>% 
  mutate(isocntry = ifelse(isocntry %in% c("DE-W", "DE-E"), "DE", isocntry)) %>% 
  group_by(qc19) %>% 
  summarise(count = n()) %>% 
  mutate(percent = count/sum(count)) %>% 
  arrange(desc(percent)) %>% 
  ungroup() %>% 
  ggplot(aes(x= qc19, y=percent, label=count)) +
  geom_col() +
  geom_text(vjust = -0.3)
```

There is a clear imbalance within the answers to qc19. We, therefore, will balance the data set in order to improve the performance of the prediction model.

## Regularization

### Correlation Check

```{r}
cor <- cor(train %>% select_if(is.numeric))

corrplot(cor, method="color", type="lower", tl.cex = 0.2)
```

### Lasso Dimension Reduction

```{r}
set.seed(5472)
# 10-fold cross-validation to select lambda
lambdas <- 10^seq(-3,5, length.out=100)

# Matrix of predictors
X <- train %>% select(-qc19) %>% as.matrix()
y <- train %>% select(qc19) %>% scale(center=TRUE, scale=FALSE) %>% as.matrix()

# Lasso with Cross-validation
lasso <- cv.glmnet(X,  y, 
                   alpha = 1, lambda = lambdas,
                   standardize = TRUE, nfolds = 10)

# Plot cv results
plot(lasso)
```

```{r}
# Best cv lambda
lambda_cv <- lasso$lambda.min

# Fit final model
model_cv <- glmnet(X, y, alpha = 1, lambda = lambda_cv, standardize = TRUE)
y_hat_cv <- predict(model_cv, X)
ssr_cv <- t(y-y_hat_cv) %*% (y - y_hat_cv)

# Check R-squared
rsq_lasso_cv <- cor(y, y_hat_cv)^2
rsq_lasso_cv
```

```{r}
lasso_coefs <- coef(model_cv)

select_features_lasso <- rownames(lasso_coefs)[coefs[,1] != 0][-1]
select_features_lasso
```

### Elastic Net Dimension Reduction

```{r message = FALSE}
set.seed(3244)
# Training control
ctrl <- trainControl(method = "repeatedcv",
                     number=10,
                     repeats = 5,
                     search = "random",
                     verboseIter = TRUE)
# Train model
enet <- train(qc19 ~., data = train, method="glmnet",
              ppreProcess = c("center", "scale"),
              tuneLength = 25,
              trControl = ctrl)

# Check R-Squared
y_hat_enet <- predict(enet, select(train, -qc19))
rsq_enet <- cor(y, y_hat_enet)^2
rsq_enet
```

```{r}
# Columns with coef = 0
enet_coefs <- coef(enet$finalModel, enet$bestTune$lambda) %>% 
  as.matrix() %>% 
  data.frame() 

select_features_enet <- enet_coefs %>% 
  filter(s1 != 0) %>% 
  row.names()
select_features_enet
```

## Feature Selection

```{r}
train_lasso <- train %>% 
  select(all_of(select_features_lasso))
```

# Performance Evaluation
